{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "import pytorch_lightning.callbacks as plc\n",
    "from model_interface import MInterface\n",
    "from data_interface import DInterface\n",
    "from src.tools.logger import SetupCallback, BestCheckpointCallback, BackupCodeCallback, TempFileCleanupCallback\n",
    "from shutil import ignore_patterns\n",
    "import pytorch_lightning.loggers as plog\n",
    "import pickle\n",
    "\n",
    "# 设置环境变量和工作目录\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.chdir(sys.path[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\n",
    "    'res_dir': './results',\n",
    "    'ex_name': 'debug',\n",
    "    'check_val_every_n_epoch': 1,\n",
    "    'dataset': 'PTM',\n",
    "    'model_name': 'MeTokenMax', # model name here\n",
    "    'lr': 1e-4,\n",
    "    'lr_scheduler': 'onecycle',\n",
    "    'offline': 1,\n",
    "    'seed': 114514,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 16,\n",
    "    'pad': 1024,\n",
    "    'min_length': 40,\n",
    "    'path': './data_inference', # data path here\n",
    "    'with_null_ptm': 1,\n",
    "    'epoch': 2,\n",
    "    'augment_eps': 0.0,\n",
    "    'module_type': 0,\n",
    "    'loss_type': 'uni',\n",
    "    'dis': 'uniform',\n",
    "    'weight_type': 0,\n",
    "    'gamma': 2.0,\n",
    "    'final_tau': 1e-4,\n",
    "    'pretrain': 0,\n",
    "    'test_only': 1,\n",
    "    'inference_pos':[[4,6],[35,56],[79,114]],\n",
    "    'ckpt_from_deepspeed': 0,\n",
    "    'ckpt_path': \"/tancheng/caozx/caozx/ProteinInvBench/results/baseline_metoken_222/checkpoints/best.ckpt\",\n",
    "    'gpus': [0],\n",
    "    'strategy': 'auto',\n",
    "    'wandb_offline': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from re import I\n",
    "from Bio import PDB\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def extract_pdb_info_from_folder(folder_path, output_json_path):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    protein_list = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdb'):\n",
    "            pdb_file_path = os.path.join(folder_path, filename)\n",
    "            structure = parser.get_structure(filename.replace('.pdb', ''), pdb_file_path)\n",
    "            \n",
    "            seq = ''\n",
    "            coords_chain_A = {'N_chain_A': [], 'C_chain_A': [], 'CA_chain_A': [], 'O_chain_A': []}\n",
    "            \n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    if chain.id == 'A':  # Process only chain A\n",
    "                        for residue in chain:\n",
    "                            if PDB.is_aa(residue):\n",
    "                                # Get the sequence\n",
    "                                seq += PDB.Polypeptide.three_to_one(residue.resname)\n",
    "                                # Get the coordinates of N, C, CA, and O atoms\n",
    "                                for atom in residue:\n",
    "                                    if atom.id == 'N':\n",
    "                                        coords_chain_A['N_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'C':\n",
    "                                        coords_chain_A['C_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'CA':\n",
    "                                        coords_chain_A['CA_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'O':\n",
    "                                        coords_chain_A['O_chain_A'].append(atom.coord.tolist())\n",
    "            \n",
    "            # Extract ID from the file name\n",
    "            pdb_id = filename.replace('.pdb', '')\n",
    "            \n",
    "            # Create the JSON object for this PDB file\n",
    "            protein_data = {\n",
    "                \"id\": pdb_id,\n",
    "                \"seq\": seq,\n",
    "                \"coords_chain_A\": coords_chain_A\n",
    "            }\n",
    "            \n",
    "            # Add to the list\n",
    "            protein_list.append(protein_data)\n",
    "    \n",
    "    # Write the JSON list to file\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(protein_list, json_file, indent=4)\n",
    "\n",
    "def process_coords_chain_A(group, coords_chain_A):\n",
    "    for key, value in coords_chain_A.items():\n",
    "        # 将非空列表转换为numpy数组并存储\n",
    "        array = np.array(value, dtype=np.float32)\n",
    "        group.create_dataset(key, data=array)\n",
    "\n",
    "def apply_ptm_indices(input_json_path, output_json_path, ptm_indices):\n",
    "    # Read the input JSON file\n",
    "    with open(input_json_path, 'r') as json_file:\n",
    "        protein_list = json.load(json_file)\n",
    "\n",
    "    for i,protein_data in enumerate(protein_list):\n",
    "        seq_length = len(protein_data[\"seq\"])\n",
    "        \n",
    "        if -1 in ptm_indices[i]:\n",
    "            ptm = [1] * seq_length\n",
    "        else:\n",
    "            ptm = [0] * seq_length\n",
    "            for index in ptm_indices[i]:\n",
    "                if 0 <= index < seq_length:\n",
    "                    ptm[index] = 1\n",
    "        \n",
    "        protein_data[\"ptm\"] = ptm\n",
    "\n",
    "    # Write the updated JSON list to file\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(protein_list, json_file, indent=4)\n",
    "\n",
    "def dict_to_hdf5(group, item):\n",
    "    for key, value in item.items():\n",
    "        if key == 'coords_chain_A':\n",
    "            coords_group = group.create_group(key)\n",
    "            process_coords_chain_A(coords_group, value)\n",
    "        elif isinstance(value, list):\n",
    "            # 处理非空列表\n",
    "            value = np.array(value)\n",
    "            group.create_dataset(key, data=value)\n",
    "        elif isinstance(value, (int, float)):\n",
    "            value = np.array([value])\n",
    "            group.create_dataset(key, data=value)\n",
    "        elif isinstance(value, str):\n",
    "            dt = h5py.special_dtype(vlen=str)\n",
    "            value = np.array([value], dtype=dt)\n",
    "            group.create_dataset(key, data=value)\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的数据类型: {type(value)}\")\n",
    "\n",
    "def json_to_hdf5(json_filepath, hdf5_filepath):\n",
    "    with open(json_filepath, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    with h5py.File(hdf5_filepath, 'w') as hdf5_file:\n",
    "        for i, item in enumerate(data):\n",
    "            group = hdf5_file.create_group(str(i))\n",
    "            dict_to_hdf5(group, item)\n",
    "            \n",
    "# Example usage\n",
    "pdb_file_path = args[\"path\"]  # Replace with your PDB file path\n",
    "output_json_path = args[\"path\"]+'/predict.json'\n",
    "output_path=args[\"path\"]+\"/predict.hdf5\"\n",
    "predict_indices=args[\"inference_pos\"]\n",
    "extract_pdb_info_from_folder(pdb_file_path, output_json_path)\n",
    "apply_ptm_indices(output_json_path,output_json_path,predict_indices)\n",
    "json_to_hdf5(output_json_path, output_path)\n",
    "print(\"Data Prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 114514\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 6ydfyzmt.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置回调函数\n",
    "def load_callbacks(args):\n",
    "    callbacks = []\n",
    "    logdir = str(os.path.join(args['res_dir'], args['ex_name']))\n",
    "    ckptdir = os.path.join(logdir, \"checkpoints\")\n",
    "    callbacks.append(BackupCodeCallback(os.path.dirname(args['res_dir']),logdir, ignore_patterns=ignore_patterns('results*', 'pdb*', 'metadata*', 'vq_dataset*', 'bin*', 'data*', '__pycache__', 'info', 'lib', 'requirements', 'debug', 'wandb')))\n",
    "    \n",
    "    metric = \"val_f1\"\n",
    "    early_stop_val = \"max\"\n",
    "    sv_filename = 'best-{epoch:02d}-{val_f1:.3f}'\n",
    "    callbacks.append(BestCheckpointCallback(\n",
    "        monitor=metric,\n",
    "        filename=sv_filename,\n",
    "        save_top_k=15,\n",
    "        mode='max',\n",
    "        save_last=True,\n",
    "        dirpath=ckptdir,\n",
    "        verbose=True,\n",
    "        every_n_epochs=args['check_val_every_n_epoch'],\n",
    "    ))\n",
    "\n",
    "    now = datetime.datetime.now().strftime(\"%m-%dT%H-%M-%S\")\n",
    "    cfgdir = os.path.join(logdir, \"configs\")\n",
    "    callbacks.append(\n",
    "        SetupCallback(\n",
    "                now=now,\n",
    "                logdir=logdir,\n",
    "                ckptdir=ckptdir,\n",
    "                cfgdir=cfgdir,\n",
    "                config=args,\n",
    "                argv_content=sys.argv + [\"gpus: {}\".format(torch.cuda.device_count())],)\n",
    "    )\n",
    "\n",
    "    callbacks.append(plc.EarlyStopping(monitor=metric, mode=early_stop_val, patience=20 if args['pretrain'] else 5))\n",
    "    callbacks.append(TempFileCleanupCallback())\n",
    "    return callbacks, ckptdir\n",
    "\n",
    "# 设置种子\n",
    "pl.seed_everything(args['seed'])\n",
    "\n",
    "# 创建数据模块\n",
    "data_module = DInterface(**args)\n",
    "data_module.setup(stage=\"predict\")\n",
    "gpu_count = torch.cuda.device_count()\n",
    "\n",
    "# 设置日志记录器\n",
    "logger = plog.WandbLogger(project='PTM-MeToken', dir='./wandb/', name=args['ex_name'], offline=args['wandb_offline'], config=args)\n",
    "\n",
    "# 加载回调函数\n",
    "callbacks, ckptdir = load_callbacks(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/MeToken/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "/root/anaconda3/envs/MeToken/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:55: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v2.0. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch_scatter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrainer_config)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 训练和测试模型\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMInterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresume_from_checkpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m model\u001b[38;5;241m.\u001b[39mhparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_indices\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# the line that comes with problem\u001b[39;00m\n\u001b[1;32m     18\u001b[0m result\u001b[38;5;241m=\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mpredict(model,data_module)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/MeToken/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:139\u001b[0m, in \u001b[0;36mModelIO.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m     67\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m     68\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MeToken/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:188\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/MeToken/lib/python3.8/site-packages/pytorch_lightning/core/saving.py:234\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 234\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "File \u001b[0;32m/tancheng/caozx/caozx/MeToken/MeToken/model_interface.py:23\u001b[0m, in \u001b[0;36mMInterface.__init__\u001b[0;34m(self, model_name, loss, lr, **kargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkargs):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters(logger\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_model()\n",
      "File \u001b[0;32m/tancheng/caozx/caozx/MeToken/MeToken/src/interface/model_interface.py:12\u001b[0m, in \u001b[0;36mMInterface_base.__init__\u001b[0;34m(self, model_name, loss, lr, **kargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_hyperparameters()\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfigure_loss()\n\u001b[1;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mres_dir, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mex_name), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/tancheng/caozx/caozx/MeToken/MeToken/model_interface.py:202\u001b[0m, in \u001b[0;36mMInterface.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m MeTokenPro_Model(params)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMeTokenMax\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetokenmax_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MeTokenMax_Model\n\u001b[1;32m    203\u001b[0m     params\u001b[38;5;241m.\u001b[39musing_metoken \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m MeTokenMax_Model(params)\n",
      "File \u001b[0;32m/tancheng/caozx/caozx/MeToken/MeToken/src/models/metokenmax_model.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, EsmModel\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gather_nodes, _dihedrals, _get_rbf, _normalize, _quaternions, _orientations_coarse_gl_tuple\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetoken_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StructureEncoder, MeTokenDecoder, MLPDecoder\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_v_direct\u001b[39m(X, eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m):\n\u001b[1;32m      9\u001b[0m     B, N \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m/tancheng/caozx/caozx/MeToken/MeToken/src/modules/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) CAIRI AI Lab. All rights reserved\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpifold_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/tancheng/caozx/caozx/MeToken/MeToken/src/modules/pifold_module.py:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_scatter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scatter_sum, scatter_softmax, scatter_mean\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdesign_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gather_nodes\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_scatter'"
     ]
    }
   ],
   "source": [
    "trainer_config = {\n",
    "    'gpus': args['gpus'] if args['ex_name'] != 'debug' else [0],\n",
    "    'max_epochs': args['epoch'],\n",
    "    'strategy': args['strategy'],\n",
    "    'accelerator': 'gpu',\n",
    "    'callbacks': callbacks,\n",
    "    'logger': logger,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    'resume_from_checkpoint': args['ckpt_path'] if args['test_only'] else None,\n",
    "}\n",
    "\n",
    "trainer = Trainer(**trainer_config)\n",
    "\n",
    "\n",
    "# 训练和测试模型\n",
    "model = MInterface.load_from_checkpoint(trainer_config[\"resume_from_checkpoint\"], strict=False)\n",
    "model.hparams[\"predict_indices\"]=args[\"inference_pos\"] # the line that comes with problem\n",
    "result=trainer.predict(model,data_module)[0]\n",
    "print(result)\n",
    "with open(\"/tancheng/caozx/caozx/ProteinInvBench/prediction_sample.pkl\",\"wb\") as f:\n",
    "    pickle.dump(result,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyMEAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
