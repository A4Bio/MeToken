{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.trainer import Trainer\n",
    "from model_interface import MInterface\n",
    "from data_interface import DInterface\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\n",
    "    'res_dir': './results',\n",
    "    'ex_name': 'debug',\n",
    "    'check_val_every_n_epoch': 1,\n",
    "    'dataset': 'PTM',\n",
    "    'model_name': 'MeToken', # model name here\n",
    "    'lr': 1e-4,\n",
    "    'lr_scheduler': 'onecycle',\n",
    "    'offline': 1,\n",
    "    'seed': 114514,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 16,\n",
    "    'pad': 1024,\n",
    "    'min_length': 40,\n",
    "    'path': './data_inference', # data path here\n",
    "    'with_null_ptm': 1,\n",
    "    'epoch': 20,\n",
    "    'augment_eps': 0.0,\n",
    "    'module_type': 94,\n",
    "    'weight_type': 0,\n",
    "    'gamma': 2.0,\n",
    "    'final_tau': 1e-4,\n",
    "    'pretrain': 0,\n",
    "    'test_only': 1,\n",
    "    'inference_pos':[[31]],\n",
    "    'ckpt_from_deepspeed': 0,\n",
    "    'ckpt_path': \"pretrained_model/checkpoint.ckpt\",\n",
    "    'gpus': [0],\n",
    "    'strategy': 'auto',\n",
    "    'wandb_offline': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from re import I\n",
    "from Bio import PDB\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def extract_pdb_info_from_folder(folder_path, output_json_path):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    protein_list = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdb'):\n",
    "            pdb_file_path = os.path.join(folder_path, filename)\n",
    "            structure = parser.get_structure(filename.replace('.pdb', ''), pdb_file_path)\n",
    "            \n",
    "            seq = ''\n",
    "            coords_chain_A = {'N_chain_A': [], 'C_chain_A': [], 'CA_chain_A': [], 'O_chain_A': []}\n",
    "            \n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    if chain.id == 'A':  # Process only chain A\n",
    "                        for residue in chain:\n",
    "                            if PDB.is_aa(residue):\n",
    "                                # Get the sequence\n",
    "                                seq += PDB.Polypeptide.three_to_one(residue.resname)\n",
    "                                # Get the coordinates of N, C, CA, and O atoms\n",
    "                                for atom in residue:\n",
    "                                    if atom.id == 'N':\n",
    "                                        coords_chain_A['N_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'C':\n",
    "                                        coords_chain_A['C_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'CA':\n",
    "                                        coords_chain_A['CA_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'O':\n",
    "                                        coords_chain_A['O_chain_A'].append(atom.coord.tolist())\n",
    "            \n",
    "            # Extract ID from the file name\n",
    "            pdb_id = filename.replace('.pdb', '')\n",
    "            \n",
    "            # Create the JSON object for this PDB file\n",
    "            protein_data = {\n",
    "                \"id\": pdb_id,\n",
    "                \"seq\": seq,\n",
    "                \"coords_chain_A\": coords_chain_A\n",
    "            }\n",
    "            \n",
    "            protein_list.append(protein_data)\n",
    "    \n",
    "    # Write the JSON list to file\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(protein_list, json_file, indent=4)\n",
    "\n",
    "def process_coords_chain_A(group, coords_chain_A):\n",
    "    for key, value in coords_chain_A.items():\n",
    "        array = np.array(value, dtype=np.float32)\n",
    "        group.create_dataset(key, data=array)\n",
    "\n",
    "def apply_ptm_indices(input_json_path, output_json_path, ptm_indices):\n",
    "    with open(input_json_path, 'r') as json_file:\n",
    "        protein_list = json.load(json_file)\n",
    "\n",
    "    for i,protein_data in enumerate(protein_list):\n",
    "        seq_length = len(protein_data[\"seq\"])\n",
    "        \n",
    "        if -1 in ptm_indices[i]:\n",
    "            ptm = [1] * seq_length\n",
    "        else:\n",
    "            ptm = [0] * seq_length\n",
    "            for index in ptm_indices[i]:\n",
    "                if 0 <= index < seq_length:\n",
    "                    ptm[index] = 1\n",
    "        \n",
    "        protein_data[\"ptm\"] = ptm\n",
    "\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(protein_list, json_file, indent=4)\n",
    "\n",
    "def dict_to_hdf5(group, item):\n",
    "    for key, value in item.items():\n",
    "        if key == 'coords_chain_A':\n",
    "            coords_group = group.create_group(key)\n",
    "            process_coords_chain_A(coords_group, value)\n",
    "        elif isinstance(value, list):\n",
    "            value = np.array(value)\n",
    "            group.create_dataset(key, data=value)\n",
    "        elif isinstance(value, (int, float)):\n",
    "            value = np.array([value])\n",
    "            group.create_dataset(key, data=value)\n",
    "        elif isinstance(value, str):\n",
    "            dt = h5py.special_dtype(vlen=str)\n",
    "            value = np.array([value], dtype=dt)\n",
    "            group.create_dataset(key, data=value)\n",
    "        else:\n",
    "            raise ValueError(f\"Data type not supported: {type(value)}\")\n",
    "\n",
    "def json_to_hdf5(json_filepath, hdf5_filepath):\n",
    "    with open(json_filepath, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    with h5py.File(hdf5_filepath, 'w') as hdf5_file:\n",
    "        for i, item in enumerate(data):\n",
    "            group = hdf5_file.create_group(str(i))\n",
    "            dict_to_hdf5(group, item)\n",
    "            \n",
    "# Example usage\n",
    "pdb_file_path = args[\"path\"]  # Replace with your PDB file path\n",
    "output_json_path = args[\"path\"]+'/predict.json'\n",
    "output_path=args[\"path\"]+\"/predict.hdf5\"\n",
    "predict_indices=args[\"inference_pos\"]\n",
    "extract_pdb_info_from_folder(pdb_file_path, output_json_path)\n",
    "apply_ptm_indices(output_json_path,output_json_path,predict_indices)\n",
    "json_to_hdf5(output_json_path, output_path)\n",
    "print(\"Data Prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DInterface(**args)\n",
    "data_module.setup(stage=\"predict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "/root/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:55: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v2.0. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "[{31: 16}]\n"
     ]
    }
   ],
   "source": [
    "trainer_config = {\n",
    "    'gpus': args['gpus'] if args['ex_name'] != 'debug' else [0],\n",
    "    'max_epochs': args['epoch'],\n",
    "    'strategy': args['strategy'],\n",
    "    'accelerator': 'gpu',\n",
    "    'resume_from_checkpoint': args['ckpt_path']\n",
    "}\n",
    "\n",
    "trainer = Trainer(**trainer_config)\n",
    "\n",
    "model = MInterface.load_from_checkpoint(trainer_config[\"resume_from_checkpoint\"], strict=False,**(args))\n",
    "model.hparams[\"predict_indices\"] = args[\"inference_pos\"] # the line that comes with problem\n",
    "result = trainer.predict(model, data_module)[0]\n",
    "print(result)\n",
    "with open(\"results/prediction_sample.pkl\",\"wb\") as f:\n",
    "    pickle.dump(result,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyMEAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
