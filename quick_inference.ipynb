{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "import pytorch_lightning.callbacks as plc\n",
    "from model_interface import MInterface\n",
    "from data_interface import DInterface\n",
    "from src.tools.logger import SetupCallback, BestCheckpointCallback, BackupCodeCallback, TempFileCleanupCallback\n",
    "from shutil import ignore_patterns\n",
    "import pytorch_lightning.loggers as plog\n",
    "import pickle\n",
    "\n",
    "# 设置环境变量和工作目录\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.chdir(sys.path[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义参数\n",
    "args = {\n",
    "    'res_dir': './results',\n",
    "    'ex_name': 'debug',\n",
    "    'check_val_every_n_epoch': 1,\n",
    "    'dataset': 'PTM',\n",
    "    'model_name': 'MeTokenMax', # model name here\n",
    "    'lr': 1e-4,\n",
    "    'lr_scheduler': 'onecycle',\n",
    "    'offline': 1,\n",
    "    'seed': 114514,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 16,\n",
    "    'pad': 1024,\n",
    "    'min_length': 40,\n",
    "    'path': './data_inference', # data path here\n",
    "    'with_null_ptm': 1,\n",
    "    'epoch': 2,\n",
    "    'augment_eps': 0.0,\n",
    "    'module_type': 0,\n",
    "    'loss_type': 'uni',\n",
    "    'dis': 'uniform',\n",
    "    'weight_type': 0,\n",
    "    'gamma': 2.0,\n",
    "    'final_tau': 1e-4,\n",
    "    'pretrain': 0,\n",
    "    'test_only': 1,\n",
    "    'inference_pos':[[4,6],[35,56],[79,114]],\n",
    "    'ckpt_from_deepspeed': 0,\n",
    "    'ckpt_path': \"/tancheng/caozx/caozx/ProteinInvBench/results/baseline_metoken_222/checkpoints/best.ckpt\",\n",
    "    'gpus': [0],\n",
    "    'strategy': 'auto',\n",
    "    'wandb_offline': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Prepared\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from re import I\n",
    "from Bio import PDB\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def extract_pdb_info_from_folder(folder_path, output_json_path):\n",
    "    parser = PDB.PDBParser(QUIET=True)\n",
    "    protein_list = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.pdb'):\n",
    "            pdb_file_path = os.path.join(folder_path, filename)\n",
    "            structure = parser.get_structure(filename.replace('.pdb', ''), pdb_file_path)\n",
    "            \n",
    "            seq = ''\n",
    "            coords_chain_A = {'N_chain_A': [], 'C_chain_A': [], 'CA_chain_A': [], 'O_chain_A': []}\n",
    "            \n",
    "            for model in structure:\n",
    "                for chain in model:\n",
    "                    if chain.id == 'A':  # Process only chain A\n",
    "                        for residue in chain:\n",
    "                            if PDB.is_aa(residue):\n",
    "                                # Get the sequence\n",
    "                                seq += PDB.Polypeptide.three_to_one(residue.resname)\n",
    "                                # Get the coordinates of N, C, CA, and O atoms\n",
    "                                for atom in residue:\n",
    "                                    if atom.id == 'N':\n",
    "                                        coords_chain_A['N_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'C':\n",
    "                                        coords_chain_A['C_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'CA':\n",
    "                                        coords_chain_A['CA_chain_A'].append(atom.coord.tolist())\n",
    "                                    elif atom.id == 'O':\n",
    "                                        coords_chain_A['O_chain_A'].append(atom.coord.tolist())\n",
    "            \n",
    "            # Extract ID from the file name\n",
    "            pdb_id = filename.replace('.pdb', '')\n",
    "            \n",
    "            # Create the JSON object for this PDB file\n",
    "            protein_data = {\n",
    "                \"id\": pdb_id,\n",
    "                \"seq\": seq,\n",
    "                \"coords_chain_A\": coords_chain_A\n",
    "            }\n",
    "            \n",
    "            # Add to the list\n",
    "            protein_list.append(protein_data)\n",
    "    \n",
    "    # Write the JSON list to file\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(protein_list, json_file, indent=4)\n",
    "\n",
    "def process_coords_chain_A(group, coords_chain_A):\n",
    "    for key, value in coords_chain_A.items():\n",
    "        # 将非空列表转换为numpy数组并存储\n",
    "        array = np.array(value, dtype=np.float32)\n",
    "        group.create_dataset(key, data=array)\n",
    "\n",
    "def apply_ptm_indices(input_json_path, output_json_path, ptm_indices):\n",
    "    # Read the input JSON file\n",
    "    with open(input_json_path, 'r') as json_file:\n",
    "        protein_list = json.load(json_file)\n",
    "\n",
    "    for i,protein_data in enumerate(protein_list):\n",
    "        seq_length = len(protein_data[\"seq\"])\n",
    "        \n",
    "        if -1 in ptm_indices[i]:\n",
    "            ptm = [1] * seq_length\n",
    "        else:\n",
    "            ptm = [0] * seq_length\n",
    "            for index in ptm_indices[i]:\n",
    "                if 0 <= index < seq_length:\n",
    "                    ptm[index] = 1\n",
    "        \n",
    "        protein_data[\"ptm\"] = ptm\n",
    "\n",
    "    # Write the updated JSON list to file\n",
    "    with open(output_json_path, 'w') as json_file:\n",
    "        json.dump(protein_list, json_file, indent=4)\n",
    "\n",
    "def dict_to_hdf5(group, item):\n",
    "    for key, value in item.items():\n",
    "        if key == 'coords_chain_A':\n",
    "            coords_group = group.create_group(key)\n",
    "            process_coords_chain_A(coords_group, value)\n",
    "        elif isinstance(value, list):\n",
    "            # 处理非空列表\n",
    "            value = np.array(value)\n",
    "            group.create_dataset(key, data=value)\n",
    "        elif isinstance(value, (int, float)):\n",
    "            value = np.array([value])\n",
    "            group.create_dataset(key, data=value)\n",
    "        elif isinstance(value, str):\n",
    "            dt = h5py.special_dtype(vlen=str)\n",
    "            value = np.array([value], dtype=dt)\n",
    "            group.create_dataset(key, data=value)\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的数据类型: {type(value)}\")\n",
    "\n",
    "def json_to_hdf5(json_filepath, hdf5_filepath):\n",
    "    with open(json_filepath, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    with h5py.File(hdf5_filepath, 'w') as hdf5_file:\n",
    "        for i, item in enumerate(data):\n",
    "            group = hdf5_file.create_group(str(i))\n",
    "            dict_to_hdf5(group, item)\n",
    "            \n",
    "# Example usage\n",
    "pdb_file_path = args[\"path\"]  # Replace with your PDB file path\n",
    "output_json_path = args[\"path\"]+'/predict.json'\n",
    "output_path=args[\"path\"]+\"/predict.hdf5\"\n",
    "predict_indices=args[\"inference_pos\"]\n",
    "extract_pdb_info_from_folder(pdb_file_path, output_json_path)\n",
    "apply_ptm_indices(output_json_path,output_json_path,predict_indices)\n",
    "json_to_hdf5(output_json_path, output_path)\n",
    "print(\"Data Prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 114514\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 5q0rxxqb.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 设置回调函数\n",
    "def load_callbacks(args):\n",
    "    callbacks = []\n",
    "    logdir = str(os.path.join(args['res_dir'], args['ex_name']))\n",
    "    ckptdir = os.path.join(logdir, \"checkpoints\")\n",
    "    callbacks.append(BackupCodeCallback(os.path.dirname(args['res_dir']),logdir, ignore_patterns=ignore_patterns('results*', 'pdb*', 'metadata*', 'vq_dataset*', 'bin*', 'data*', '__pycache__', 'info', 'lib', 'requirements', 'debug', 'wandb')))\n",
    "    \n",
    "    metric = \"val_f1\"\n",
    "    early_stop_val = \"max\"\n",
    "    sv_filename = 'best-{epoch:02d}-{val_f1:.3f}'\n",
    "    callbacks.append(BestCheckpointCallback(\n",
    "        monitor=metric,\n",
    "        filename=sv_filename,\n",
    "        save_top_k=15,\n",
    "        mode='max',\n",
    "        save_last=True,\n",
    "        dirpath=ckptdir,\n",
    "        verbose=True,\n",
    "        every_n_epochs=args['check_val_every_n_epoch'],\n",
    "    ))\n",
    "\n",
    "    now = datetime.datetime.now().strftime(\"%m-%dT%H-%M-%S\")\n",
    "    cfgdir = os.path.join(logdir, \"configs\")\n",
    "    callbacks.append(\n",
    "        SetupCallback(\n",
    "                now=now,\n",
    "                logdir=logdir,\n",
    "                ckptdir=ckptdir,\n",
    "                cfgdir=cfgdir,\n",
    "                config=args,\n",
    "                argv_content=sys.argv + [\"gpus: {}\".format(torch.cuda.device_count())],)\n",
    "    )\n",
    "\n",
    "    callbacks.append(plc.EarlyStopping(monitor=metric, mode=early_stop_val, patience=20 if args['pretrain'] else 5))\n",
    "    callbacks.append(TempFileCleanupCallback())\n",
    "    return callbacks, ckptdir\n",
    "\n",
    "# 设置种子\n",
    "pl.seed_everything(args['seed'])\n",
    "\n",
    "# 创建数据模块\n",
    "data_module = DInterface(**args)\n",
    "data_module.setup(stage=\"predict\")\n",
    "gpu_count = torch.cuda.device_count()\n",
    "\n",
    "# 设置日志记录器\n",
    "logger = plog.WandbLogger(project='PTM-MeToken', dir='./wandb/', name=args['ex_name'], offline=args['wandb_offline'], config=args)\n",
    "\n",
    "# 加载回调函数\n",
    "callbacks, ckptdir = load_callbacks(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s]\n",
      "[{4: 24, 6: 1}, {35: 24, 56: 1}, {79: 24, 114: 1}]\n"
     ]
    }
   ],
   "source": [
    "trainer_config = {\n",
    "    'gpus': args['gpus'] if args['ex_name'] != 'debug' else [0],\n",
    "    'max_epochs': args['epoch'],\n",
    "    'strategy': args['strategy'],\n",
    "    'accelerator': 'gpu',\n",
    "    'callbacks': callbacks,\n",
    "    'logger': logger,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    'resume_from_checkpoint': args['ckpt_path'] if args['test_only'] else None,\n",
    "}\n",
    "\n",
    "trainer = Trainer(**trainer_config)\n",
    "\n",
    "\n",
    "# 训练和测试模型\n",
    "model = MInterface.load_from_checkpoint(trainer_config[\"resume_from_checkpoint\"], strict=False)\n",
    "model.hparams[\"predict_indices\"]=args[\"inference_pos\"] # the line that comes with problem\n",
    "result=trainer.predict(model,data_module)[0]\n",
    "print(result)\n",
    "with open(\"/tancheng/caozx/caozx/ProteinInvBench/prediction_sample.pkl\",\"wb\") as f:\n",
    "    pickle.dump(result,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyMEAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
