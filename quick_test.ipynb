{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "import pytorch_lightning.callbacks as plc\n",
    "from model_interface import MInterface\n",
    "from data_interface import DInterface\n",
    "from src.tools.logger import SetupCallback, BestCheckpointCallback, BackupCodeCallback, TempFileCleanupCallback\n",
    "from shutil import ignore_patterns\n",
    "import pytorch_lightning.loggers as plog\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'res_dir': './results',\n",
    "    'ex_name': 'debug',\n",
    "    'check_val_every_n_epoch': 1,\n",
    "    'dataset': 'PTM',\n",
    "    'model_name': 'MeToken', # model name here\n",
    "    'lr': 1e-4,\n",
    "    'lr_scheduler': 'onecycle',\n",
    "    'offline': 1,\n",
    "    'seed': 114514,\n",
    "    'batch_size': 16,\n",
    "    'num_workers': 16,\n",
    "    'pad': 1024,\n",
    "    'min_length': 40,\n",
    "    'path': './data_test/generalization/qPTM_dataset/', # data path here\n",
    "    'with_null_ptm': 0,\n",
    "    'epoch': 20,\n",
    "    'augment_eps': 0.0,\n",
    "    'module_type': 94,\n",
    "    'weight_type': 0,\n",
    "    'gamma': 2.0,\n",
    "    'final_tau': 1e-4,\n",
    "    'pretrain': 0,\n",
    "    'test_only': 1,\n",
    "    'inference_pos':None,\n",
    "    'ckpt_from_deepspeed': 0,\n",
    "    'ckpt_path': \"pretrained_model/checkpoint.ckpt\",\n",
    "    'gpus': [0],\n",
    "    'strategy': 'auto',\n",
    "    'wandb_offline': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 114514\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id dztpcgrl.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_callbacks(args):\n",
    "    callbacks = []\n",
    "    logdir = str(os.path.join(args['res_dir'], args['ex_name']))\n",
    "    ckptdir = os.path.join(logdir, \"checkpoints\")\n",
    "    callbacks.append(BackupCodeCallback(os.path.dirname(args['res_dir']),logdir, ignore_patterns=ignore_patterns('results*', 'pdb*', 'metadata*', 'vq_dataset*', 'bin*', 'data*', '__pycache__', 'info', 'lib', 'requirements', 'debug', 'wandb')))\n",
    "    \n",
    "    metric = \"val_f1\"\n",
    "    early_stop_val = \"max\"\n",
    "    sv_filename = 'best-{epoch:02d}-{val_f1:.3f}'\n",
    "    callbacks.append(BestCheckpointCallback(\n",
    "        monitor=metric,\n",
    "        filename=sv_filename,\n",
    "        save_top_k=15,\n",
    "        mode='max',\n",
    "        save_last=True,\n",
    "        dirpath=ckptdir,\n",
    "        verbose=True,\n",
    "        every_n_epochs=args['check_val_every_n_epoch'],\n",
    "    ))\n",
    "\n",
    "    now = datetime.datetime.now().strftime(\"%m-%dT%H-%M-%S\")\n",
    "    cfgdir = os.path.join(logdir, \"configs\")\n",
    "    callbacks.append(\n",
    "        SetupCallback(\n",
    "                now=now,\n",
    "                logdir=logdir,\n",
    "                ckptdir=ckptdir,\n",
    "                cfgdir=cfgdir,\n",
    "                config=args,\n",
    "                argv_content=sys.argv + [\"gpus: {}\".format(torch.cuda.device_count())],)\n",
    "    )\n",
    "\n",
    "    callbacks.append(plc.EarlyStopping(monitor=metric, mode=early_stop_val, patience=20 if args['pretrain'] else 5))\n",
    "    callbacks.append(TempFileCleanupCallback())\n",
    "    return callbacks, ckptdir\n",
    "\n",
    "pl.seed_everything(args['seed'])\n",
    "\n",
    "data_module = DInterface(**args)\n",
    "data_module.setup(stage=\"test\")\n",
    "gpu_count = torch.cuda.device_count()\n",
    "\n",
    "logger = plog.WandbLogger(project='PTM-MeToken', dir='./wandb/', name=args['ex_name'], offline=args['wandb_offline'], config=args)\n",
    "\n",
    "callbacks, ckptdir = load_callbacks(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "/root/anaconda3/envs/dyMEAN/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:55: LightningDeprecationWarning: Setting `Trainer(resume_from_checkpoint=)` is deprecated in v1.5 and will be removed in v2.0. Please pass `Trainer.fit(ckpt_path=)` directly instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100-SXM4-80GB') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 27/27 [00:28<00:00,  1.05s/it]\n",
      "accuracy: 0.8979, precision: 0.8588, recall: 0.6875, f1 score: 0.7241, mcc score: 0.7998, auroc: 0.9605, auprc: 0.6836\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "trainer_config = {\n",
    "    'gpus': args['gpus'] if args['ex_name'] != 'debug' else [0],\n",
    "    'max_epochs': args['epoch'],\n",
    "    'strategy': args['strategy'],\n",
    "    'accelerator': 'gpu',\n",
    "    'callbacks': callbacks,\n",
    "    'logger': logger,\n",
    "    'gradient_clip_val': 1.0,\n",
    "    'resume_from_checkpoint': args['ckpt_path']\n",
    "}\n",
    "\n",
    "trainer = Trainer(**trainer_config)\n",
    "model = MInterface.load_from_checkpoint(trainer_config[\"resume_from_checkpoint\"], strict=False,model_name=args[\"model_name\"])\n",
    "trainer.test(model,data_module)\n",
    "if trainer.global_rank == 0:\n",
    "    metrics = model.cal_metric(path=args[\"path\"])\n",
    "    with open(os.path.join(args[\"res_dir\"], args[\"ex_name\"], 'metrics.json'), 'w') as file_obj:\n",
    "        json.dump(metrics, file_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dyMEAN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
